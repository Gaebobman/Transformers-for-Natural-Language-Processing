{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : 입력 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x = np.array([[1.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 2.0, 0.0, 2.0],\n",
    "              [1.0, 1.0, 1.0, 1.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: 가중치 행렬 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_mode=4\n",
      "w_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n",
      "w_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "w_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Weight Matrice\n",
    "# Q_w: For training Query\n",
    "# K_w: For training Key\n",
    "# V_w: For training Value\n",
    "\n",
    "print(\"Step 2: weights 3 dimensions x d_mode=4\")\n",
    "print(\"w_query\")\n",
    "w_query = np.array([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]])\n",
    "print(w_query)\n",
    "\n",
    "print(\"w_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "print(w_key)\n",
    "\n",
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "print(w_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Q, K, V를 얻기 위한 행렬곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q, K, V\n",
      "Query: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n",
      "\n",
      "Key: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n",
      "\n",
      "Value: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q, K, V\")\n",
    "print(\"Query: x * w_query\")\n",
    "Q = np.matmul(x, w_query)\n",
    "print(Q)\n",
    "\n",
    "print(\"\\nKey: x * w_key\")\n",
    "K = np.matmul(x, w_key)\n",
    "print(K)\n",
    "\n",
    "print(\"\\nValue: x * w_value\")\n",
    "V = np.matmul(x, w_value)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "# We floor d_k(=root(3) == 1.73) to 1 \n",
    "d_k = 1\n",
    "attention_scores = (Q @ K.transpose()) /d_k\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.25010005 0.37494998 0.37494998]\n",
      "[0.2133134  0.56950313 0.21718348]\n",
      "[0.22037557 0.53143172 0.24819272]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0] = softmax(attention_scores[0])\n",
    "attention_scores[1] = softmax(attention_scores[1])\n",
    "attention_scores[2] = softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/d_k * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1\n",
      "[0.25010005 0.5002001  0.75030014]\n",
      "Attention 2\n",
      "[0.74989995 2.99959981 0.        ]\n",
      "Attention 3\n",
      "[0.74989995 2.24969986 1.12484993]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/d_k * V\") \n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "\n",
    "print(\"Attention 1\")\n",
    "attention1 = attention_scores[0].reshape(-1, 1)\n",
    "attention1 = attention_scores[0][0] * V[0]\n",
    "print(attention1)\n",
    "\n",
    "print(\"Attention 2\")\n",
    "attention2 = attention_scores[1].reshape(-1, 1)\n",
    "attention2 = attention_scores[0][1] * V[1]\n",
    "print(attention2)\n",
    "\n",
    "print(\"Attention 3\")\n",
    "attention3 = attention_scores[2].reshape(-1, 1)\n",
    "attention3 = attention_scores[0][2] * V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Summed the results to create the first line of the output matrix\n",
      "[1.74989995 5.74949976 1.87515007]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: Summed the results to create the first line of the output matrix\")\n",
    "attention_input1 = attention1 + attention2 + attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.5970748  0.36367763 0.43239902 0.50020547 0.04042945 0.70879529\n",
      "  0.92513627 0.76318703 0.82708775 0.73484174 0.90595519 0.11480428\n",
      "  0.20385901 0.74245524 0.05028901 0.12783404 0.15433177 0.05920594\n",
      "  0.01936851 0.53322801 0.74184568 0.7669367  0.64714052 0.62261756\n",
      "  0.50542523 0.0400218  0.59374091 0.49236715 0.41725364 0.3636184\n",
      "  0.21779469 0.1904669  0.22841884 0.75088335 0.59793212 0.06398795\n",
      "  0.40317017 0.81376082 0.01989627 0.65705826 0.84048557 0.84560705\n",
      "  0.69090466 0.74157831 0.7843926  0.81812891 0.11928947 0.17621668\n",
      "  0.84839761 0.26591421 0.31259511 0.65462363 0.79157665 0.09330133\n",
      "  0.75513028 0.65724665 0.62089579 0.90309095 0.69273112 0.06349903\n",
      "  0.05366196 0.57612369 0.48392781 0.20870632]\n",
      " [0.2821444  0.89579449 0.44870095 0.05965957 0.69945599 0.60883746\n",
      "  0.21746735 0.03466307 0.0032486  0.80601401 0.28667807 0.05168512\n",
      "  0.55995755 0.71482176 0.89994773 0.84352259 0.70612986 0.52474358\n",
      "  0.91015639 0.51168316 0.41926592 0.71955748 0.2502995  0.68566121\n",
      "  0.36064801 0.17558052 0.20791049 0.16213067 0.18030954 0.51345518\n",
      "  0.21283232 0.8257371  0.74617156 0.15340118 0.38456442 0.66040352\n",
      "  0.57489566 0.93336455 0.54779793 0.10661186 0.96196404 0.63334395\n",
      "  0.43187143 0.37644577 0.67576191 0.77662915 0.54773105 0.17538678\n",
      "  0.6669827  0.95056971 0.93620358 0.41015039 0.02208054 0.94025089\n",
      "  0.23646907 0.55982325 0.17767985 0.24347555 0.96323164 0.46837796\n",
      "  0.50306928 0.82386748 0.86823484 0.11141451]\n",
      " [0.18637428 0.71272898 0.30040257 0.10520217 0.55983689 0.63408227\n",
      "  0.66506804 0.93191065 0.64519727 0.20480553 0.23465581 0.24431823\n",
      "  0.16043786 0.56007167 0.60045838 0.19614878 0.6598102  0.306089\n",
      "  0.15523538 0.85419246 0.35002169 0.94685699 0.36791768 0.53468832\n",
      "  0.32053266 0.41190414 0.45652295 0.61732515 0.86998429 0.13452418\n",
      "  0.4060779  0.65244484 0.78354701 0.53765187 0.48789586 0.19557595\n",
      "  0.88619439 0.67170961 0.48922429 0.38348222 0.52577864 0.59243301\n",
      "  0.54977324 0.74559833 0.8918805  0.94006993 0.11341913 0.98096996\n",
      "  0.71583308 0.42182631 0.47044372 0.70542516 0.79516109 0.16442548\n",
      "  0.75654231 0.60395423 0.67430757 0.55162597 0.19153989 0.03683032\n",
      "  0.70844508 0.25531614 0.4370339  0.38631629]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "#We assume we have 3 results with learned weights (they were not trained in this example)\n",
    "#We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n",
      "[[0.66966339 0.21716048 0.6784728  ... 0.85023941 0.46794025 0.02574062]\n",
      " [0.94651686 0.3888484  0.03854622 ... 0.32152209 0.48739146 0.95762426]\n",
      " [0.25993755 0.12366173 0.87285598 ... 0.26282705 0.58397431 0.26263141]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\")\n",
    "output_attention = np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Il est facile de traduire des langues à l'aide de transformateurs\"}]\n"
     ]
    }
   ],
   "source": [
    "#@title Retrieve pipeline of modules and choose English to French translation\n",
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"It is easy to translate languages with transformers\", max_length=40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
