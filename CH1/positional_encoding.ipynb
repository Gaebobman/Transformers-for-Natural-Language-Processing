{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/plumson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints outputs if set to 1, default = 0\n",
    "dprint=0\n",
    "\n",
    "sample = open(\"text.txt\", \"r\")\n",
    "s = sample.read()\n",
    "\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Sentence parsing\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp)\n",
    "\n",
    "# Creating Skip Gram model \n",
    "#model2 = gensim.models.Word2Vec(data, min_count = 1, size = 512,window = 5, sg = 1) \n",
    "#model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size= 512, window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1, word2 = 'black', 'brown' \n",
    "pos1, pos2 = 2, 10\n",
    "a = model2.wv[word1]\n",
    "b = model2.wv[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(dprint == 1):\n",
    "    print(a)\n",
    "    \n",
    "# Compute Cosine similarity\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    "\n",
    "aa = a.reshape(1, 512)\n",
    "ba = b.reshape(1, 512)\n",
    "cos_lib = cosine_similarity(aa, ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe1 = aa.copy()\n",
    "pe2 = aa.copy()\n",
    "pe3 = aa.copy()\n",
    "paa = aa.copy()\n",
    "pba = ba.copy()\n",
    "d_model = 512\n",
    "max_print = d_model\n",
    "max_length = 20\n",
    "\n",
    "for i in range(0, max_print, 2):\n",
    "    pe1[0][i] = math.sin(pos1 / (10000 ** ((2 * i)/d_model)))\n",
    "    paa[0][i] = (paa[0][i]*math.sqrt(d_model))+ pe1[0][i]\n",
    "    pe1[0][i + 1] = math.cos(pos1 / (10000 ** ((2 * i)/d_model)))\n",
    "    paa[0][i + 1] = (paa[0][i + 1] * math.sqrt(d_model)) * pe1[0][i + 1]\n",
    "    if dprint == 1:\n",
    "        print(i,pe1[0][i],i+1,pe1[0][i+1])\n",
    "        print(i,paa[0][i],i+1,paa[0][i+1])\n",
    "        print(\"\\n\")\n",
    "\n",
    "max_len = max_length\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/ d_model)) \n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black brown\n",
      "[[0.99951637]] word similarity\n",
      "[[0.8600013]] positional similarity\n",
      "[[0.8371738]] positional encoding similarity\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, max_print,2):\n",
    "                pe2[0][i] = math.sin(pos2 / (10000 ** ((2 * i)/d_model)))\n",
    "                pba[0][i] = (pba[0][i]*math.sqrt(d_model))+ pe2[0][i]\n",
    "            \n",
    "                pe2[0][i+1] = math.cos(pos2 / (10000 ** ((2 * i)/d_model)))\n",
    "                pba[0][i+1] = (pba[0][i+1]*math.sqrt(d_model))+ pe2[0][i+1]\n",
    "               \n",
    "                if dprint==1:\n",
    "                        print(i,pe2[0][i],i+1,pe2[0][i+1])\n",
    "                        print(i,paa[0][i],i+1,paa[0][i+1])\n",
    "                        print(\"\\n\")\n",
    "print(word1,word2)\n",
    "cos_lib = cosine_similarity(aa, ba)\n",
    "print(cos_lib,\"word similarity\")\n",
    "cos_lib = cosine_similarity(pe1, pe2)\n",
    "print(cos_lib,\"positional similarity\")\n",
    "cos_lib = cosine_similarity(paa, pba)\n",
    "print(cos_lib,\"positional encoding similarity\")\n",
    "\n",
    "\n",
    "if dprint==1:\n",
    "        print(word1)\n",
    "        print(\"embedding\")\n",
    "        print(aa)\n",
    "        print(\"positional encoding\")\n",
    "        print(pe1)\n",
    "        print(\"encoded embedding\")\n",
    "        print(paa)\n",
    "\n",
    "        print(word2)\n",
    "        print(\"embedding\")\n",
    "        print(ba)\n",
    "        print(\"positional encoding\")\n",
    "        print(pe2)\n",
    "        print(\"encoded embedding\")\n",
    "        print(pba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
