{"cells":[{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# How to train a new language model from scratch using Transformers and Tokenizers\n","\n","Copyright 2020, Denis Rothman. Denis Rothman adapted a Hugging Face reference notebook to pretrain a transformer model.The next steps would be work on the building a larger dataset and testing several transformer models. \n","\n","The Transformer model of this Notebook is a Transformer model named ***KantaiBERT***. ***KantaiBERT*** is trained as a RoBERTa Transformer with DistilBERT architecture. The dataset was compiled with three books by Immanuel Kant downloaded from the [Gutenberg Project](https://www.gutenberg.org/). \n","\n","<img src=\"https://eco-ai-horizons.com/data/Kant.jpg\" style=\"margin: auto; display: block; width: 260px;\">\n","\n","![](https://commons.wikimedia.org/wiki/Kant_gemaelde_1.jpg)\n","\n","***KantaiBERT*** was pretrained with a small model of 84 million parameters using the same number of layers and heads as DistilBert, i.e., 6 layers, 768 hidden size,and 12 attention heads. ***KantaiBERT*** is then fine-tuned for a downstream masked Language Modeling task.\n","\n","### The Hugging Face original Reference and notes:\n","\n","Notebook edition (link to original of the reference blogpost [link](https://huggingface.co/blog/how-to-train)).\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2860,"status":"ok","timestamp":1611303247694,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"IMnymRDLe0hi","outputId":"706de1c8-715a-41e2-bdcf-3caa67125bf8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["#@title Step 3: Training a Tokenizer\n","from pathlib import Path\n","from tokenizers import ByteLevelBPETokenizer\n","\n","paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n","# Initialize a tokenizer\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n","    \"<s>\",\n","    \"<pad>\",\n","    \"</s>\",\n","    \"<unk>\",\n","    \"<mask>\",\n","])\n","\n","    # \"<s>\",      start token\n","    # \"<pad>\",    padding token\n","    # \"</s>\",     ebd token\n","    # \"<unk>\",    unknown token\n","    # \"<mask>\",   mask token\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1506,"status":"ok","timestamp":1611303250245,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"nqYKX1XYyRI-","outputId":"3247a100-2230-4c7f-92e2-41442e76f3b9"},"outputs":[{"data":{"text/plain":["['../content/KantaiBERT/vocab.json', '../content/KantaiBERT/merges.txt']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["#@title Step 4: Saving the files to disk\n","import os\n","token_dir = '../content/KantaiBERT'\n","if not os.path.exists(token_dir):\n","  os.makedirs(token_dir)\n","tokenizer.save_model(token_dir)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tKVWB8WShT-z"},"outputs":[],"source":["#@title Step 5 Loading the Trained Tokenizer Files \n","from tokenizers.implementations import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","tokenizer = ByteLevelBPETokenizer(\n","    \"../content/KantaiBERT/vocab.json\",\n","    \"../content/KantaiBERT/merges.txt\",\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1393,"status":"ok","timestamp":1611303257943,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"I9hQqVS_qZWg","outputId":"ed5a7467-b61e-4210-d2e5-c506edd44268"},"outputs":[{"data":{"text/plain":["['The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode(\"The Critique of Pure Reason.\").tokens"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hO5M3vrAhcuj"},"outputs":[],"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1499,"status":"ok","timestamp":1611303260078,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"OGjAwZVGrfyS","outputId":"fa7923d2-939c-485a-a064-fb43966357cc"},"outputs":[{"data":{"text/plain":["Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode(\"The Critique of Pure Reason.\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1562,"status":"ok","timestamp":1611303382926,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"VNZZs-r6iKAV","outputId":"6b3f8b32-4ccd-4661-d58a-74b324766495"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/plumson/anaconda3/envs/transformers2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#@title Checking that PyTorch Sees CUDAnot\n","import torch\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LTXXutqeDzPi"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'transformers'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Step 7: Defining the configuration of the Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaConfig\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m RobertaConfig(\n\u001b[1;32m      5\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m52_000\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_position_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m514\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     type_vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"]}],"source":["#@title Step 7: Defining the configuration of the Model\n","from transformers import RobertaConfig\n","\n","config = RobertaConfig(\n","    vocab_size=52_000,\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1631,"status":"ok","timestamp":1611303394881,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"5-UsuK9Ps0H7","outputId":"405400e1-733f-490b-de7b-ae249d95ac01"},"outputs":[],"source":["print(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4keFBUjQFOD1"},"outputs":[],"source":["#@title Step 8: Re-creating the Tokenizer in Transformers\n","from transformers import RobertaTokenizer\n","tokenizer = RobertaTokenizer.from_pretrained(\"../content/KantaiBERT\", max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4263,"status":"ok","timestamp":1611303404170,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"BzMqR-dzF4Ro","outputId":"e71ab069-0d78-4592-f4cc-158050b47f75"},"outputs":[],"source":["#@title Step 9: Initializing a Model From Scratch\n","from transformers import RobertaForMaskedLM\n","\n","model = RobertaForMaskedLM(config=config)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1417,"status":"ok","timestamp":1611303407295,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"jU6JhBSTKiaM","outputId":"7cefac4a-263c-4785-d91f-f9020bfd3d1c"},"outputs":[],"source":["print(model.num_parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1327,"status":"ok","timestamp":1611303409350,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"-BXhhe7twTxb","outputId":"5f78d978-6e80-477e-aebc-3a1c9119b5e9"},"outputs":[],"source":["#@title Exploring the Parameters\n","LP=list(model.parameters())\n","lp=len(LP)\n","print(lp)\n","for p in range(0,lp):\n","  print(LP[p])                         "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1596,"status":"ok","timestamp":1611303413503,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"Ej82kG6K3akQ","outputId":"09db7a80-a3c2-4cfb-9a80-dd172a1bffa1"},"outputs":[],"source":["#@title Counting the parameters\n","np=0\n","for p in range(0,lp):#number of tensors\n","  PL2=True\n","  try:\n","    L2=len(LP[p][0]) #check if 2D\n","  except:\n","    L2=1             #not 2D but 1D\n","    PL2=False\n","  L1=len(LP[p])      \n","  L3=L1*L2\n","  np+=L3             # number of parameters per tensor\n","  if PL2==True:\n","    print(p,L1,L2,L3)  # displaying the sizes of the parameters\n","  if PL2==False:\n","    print(p,L1,L3)  # displaying the sizes of the parameters\n","\n","print(np)              # total number of parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22936,"status":"ok","timestamp":1611303439451,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"GlvP_A-THEEl","outputId":"ce117d0d-56d0-473f-eb4e-9efffc7b25dc"},"outputs":[],"source":["#@title Step 10: Building the Dataset\n","from transformers import LineByLineTextDataset\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"./kant.txt\",\n","    block_size=128,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTgWPa9Dipk2"},"outputs":[],"source":["#@title Step 11: Defining a Data Collator\n","from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpvnFFmZJD-N"},"outputs":[],"source":["#@title Step 12: Initializing the Trainer\n","from transformers import Trainer, TrainingArguments\n","training_args = TrainingArguments(\n","    output_dir=\"../content/KantaiBERT\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=64,\n","    save_steps=10_000,\n","    save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"executionInfo":{"elapsed":351691,"status":"ok","timestamp":1611303814910,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"VmaHZXzmkNtJ","outputId":"998acefe-df4b-4d07-8059-d25b323587e1"},"outputs":[],"source":["#@title Step 13: Pre-training the Model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDNgPls7_l13"},"outputs":[],"source":["#@title Step 14: Saving the Final Model(+tokenizer + config) to disk\n","trainer.save_model(\"../content/KantaiBERT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6144,"status":"ok","timestamp":1611304118693,"user":{"displayName":"Karan Sonawane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64","userId":"05479461208077736330"},"user_tz":-330},"id":"ltXgXyCbAJLY","outputId":"36ddf0da-9b07-4f97-b8ad-834827e4bc25"},"outputs":[],"source":["#@title Step 15: Language Modeling with the FillMaskPipeline\n","from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=\"../content/KantaiBERT\",\n","    tokenizer=\"../content/KantaiBERT\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UIvgZ3S6AO0z","outputId":"19c8ae6c-b5b3-4be9-c84e-772fabc5a5c9"},"outputs":[],"source":["fill_mask(\"Human thinking involves<mask>.\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"KantaiBERT_2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
